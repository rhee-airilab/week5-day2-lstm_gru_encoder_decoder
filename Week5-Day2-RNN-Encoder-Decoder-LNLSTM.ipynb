{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - Encoder-Decoder Model (with LNLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:28.712517Z",
     "start_time": "2017-10-11T01:45:28.480479Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -fr logdir4\n",
    "!mkdir -p logdir4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.370534Z",
     "start_time": "2017-10-11T01:45:28.713749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@media print {\n",
       "  a[href]:after {\n",
       "    content: none !important;\n",
       "  }\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext do_not_print_href\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set - symbols & symbol map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력/출력 문자는 0~9 까지의 숫자, 연산자 `'+'`, `' '` (END), `'='` (START) 포함하여 13 종류\n",
    "\n",
    "- 사용가능한 연산자는 `'+'`\n",
    "\n",
    "- 문자(symbol) 와 해당 문자의 인덱스 넘버 사이의 변환을 위한 배열/사전 준비\n",
    "\n",
    "  - `symbols[]`    : 인덱스에서 문자로\n",
    "  - `symbol_map[]` : 문자에서 인덱스로\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.380376Z",
     "start_time": "2017-10-11T01:45:29.374030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symbols        = [' ', '0', '1', '2', '3', '4', '5',\n",
    "                  '6', '7', '8', '9', '+', '=']\n",
    "operators      = ['+']\n",
    "\n",
    "symbol_map     = {s: i \\\n",
    "                  for i,s in enumerate(symbols)}\n",
    "input_units    = output_units    = len(symbol_map)\n",
    "hidden_units   = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.393602Z",
     "start_time": "2017-10-11T01:45:29.381594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol_map = {' ': 0, '+': 11, '1': 2, '0': 1, '3': 4, '2': 3, '5': 6, '4': 5, '7': 8, '6': 7, '9': 10, '8': 9, '=': 12}\n"
     ]
    }
   ],
   "source": [
    "print('symbol_map =',symbol_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set - Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.402041Z",
     "start_time": "2017-10-11T01:45:29.396723Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_random_data():\n",
    "    t1        = str(np.random.randint(1000))\n",
    "    op        = np.random.choice(operators)\n",
    "    t2        = str(np.random.randint(1000))\n",
    "    \n",
    "    expr      = t1 + op + t2\n",
    "    ans       = '='+str(eval(expr))+' '\n",
    "    \n",
    "    return expr, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.414660Z",
     "start_time": "2017-10-11T01:45:29.403871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['3', '6', '4', '+', '2', '0', '1'], ['=', '5', '6', '5', ' '])\n",
      "(['3', '0', '2', '+', '1', '7', '9'], ['=', '4', '8', '1', ' '])\n",
      "(['1', '8', '8', '+', '3', '5', '0'], ['=', '5', '3', '8', ' '])\n",
      "(['7', '7', '7', '+', '3', '8', '4'], ['=', '1', '1', '6', '1', ' '])\n",
      "(['3', '9', '1', '+', '8', '7', '9'], ['=', '1', '2', '7', '0', ' '])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    expr, ans = make_random_data()\n",
    "    print(([c for c in expr], [c for c in ans]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set - one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.425767Z",
     "start_time": "2017-10-11T01:45:29.416048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(n):\n",
    "    \"\"\"\n",
    "    3 ==> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "    res = np.zeros(13, dtype=np.float32)\n",
    "    res[n] = 1.0\n",
    "    return res\n",
    "\n",
    "def arg_max(v):\n",
    "    \"\"\"\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] ==> 3\n",
    "    \"\"\"\n",
    "    return np.argmax(v, axis=-1)\n",
    "\n",
    "# test\n",
    "assert 7 == arg_max(one_hot(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.435656Z",
     "start_time": "2017-10-11T01:45:29.428663Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_max_seq_len = 7\n",
    "decoder_max_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.448895Z",
     "start_time": "2017-10-11T01:45:29.436987Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str_to_onehot(str, max_seq_len):\n",
    "    buf           = np.zeros([max_seq_len,input_units])\n",
    "    buf          += \\\n",
    "      one_hot(symbol_map[' ']).reshape([1,-1]) # <<<===\n",
    "    seq_len       = len(str)\n",
    "    buf[:seq_len] = [one_hot(symbol_map[c]) for c in str]\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.461386Z",
     "start_time": "2017-10-11T01:45:29.450114Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot_to_str(data, data_len):\n",
    "    return ''.join([symbols[v] \\\n",
    "                    for v in arg_max(data)][:data_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.470585Z",
     "start_time": "2017-10-11T01:45:29.465736Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_data(expr, ans):\n",
    "    e_seq_len         = len(expr)\n",
    "    e_in              = str_to_onehot(expr,\n",
    "                                      encoder_max_seq_len)\n",
    "    d_seq_len         = len(ans) - 1\n",
    "    d_in              = str_to_onehot(ans[:-1],\n",
    "                                      decoder_max_seq_len)\n",
    "    d_out             = str_to_onehot(ans[1:],\n",
    "                                      decoder_max_seq_len)\n",
    "    return e_seq_len, e_in, d_seq_len, d_in, d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.479701Z",
     "start_time": "2017-10-11T01:45:29.473777Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_data(e_len, e_in, d_len, d_in, d_out):\n",
    "    return  e_len, \\\n",
    "            onehot_to_str(e_in, e_len), \\\n",
    "            d_len, \\\n",
    "            onehot_to_str(d_in, d_len), \\\n",
    "            onehot_to_str(d_out, d_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.489420Z",
     "start_time": "2017-10-11T01:45:29.481005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, '5+969', 4, '=974', '974 ')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_data(*encode_data(*make_random_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set: create data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.501075Z",
     "start_time": "2017-10-11T01:45:29.491750Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:29.512670Z",
     "start_time": "2017-10-11T01:45:29.502266Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_num_data = 60000\n",
    "test_num_data  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.319507Z",
     "start_time": "2017-10-11T01:45:29.513932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common data format\n",
    "# e_len, e_in, d_len, d_in, d_out\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.encoder_seq_len  = []\n",
    "        self.encoder_in_data  = []\n",
    "        self.decoder_seq_len  = []\n",
    "        self.decoder_in_data  = []\n",
    "        self.decoder_out_data = []\n",
    "        \n",
    "    def append(self, t):\n",
    "        self.encoder_seq_len.append(t[0])\n",
    "        self.encoder_in_data.append(t[1])\n",
    "        self.decoder_seq_len.append(t[2])\n",
    "        self.decoder_in_data.append(t[3])\n",
    "        self.decoder_out_data.append(t[4])\n",
    "        \n",
    "    def next_batch(self,batch_size=BATCH_SIZE):\n",
    "        data_len = len(self.encoder_seq_len)\n",
    "        batch_pointer = 0\n",
    "        while batch_pointer + batch_size <= data_len:\n",
    "            ss   = np.random.randint(\n",
    "                data_len - batch_size - 1)\n",
    "            yield \\\n",
    "                self.encoder_seq_len[ss:ss+batch_size], \\\n",
    "                self.encoder_in_data[ss:ss+batch_size], \\\n",
    "                self.decoder_seq_len[ss:ss+batch_size], \\\n",
    "                self.decoder_in_data[ss:ss+batch_size], \\\n",
    "                self.decoder_out_data[ss:ss+batch_size]\n",
    "            batch_pointer += batch_size\n",
    "\n",
    "\n",
    "np.random.seed(37L)\n",
    "\n",
    "train_data = Dataset()\n",
    "for i in range(train_num_data):\n",
    "    expr, ans     = make_random_data()\n",
    "    train_data.append(encode_data(expr, ans))\n",
    "\n",
    "test_data  = Dataset()\n",
    "for i in range(test_num_data):\n",
    "    expr, ans     = make_random_data()\n",
    "    test_data.append(encode_data(expr, ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data set: data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print & verify first training data\n",
    "\n",
    "- `next_batch()` 는 `for` 문장과 함께 쓰일 수 있음.\n",
    "- `next_batch()` 가 `for` 문장과 함께 쓰지지 않는 경우는 다시 next() 를 호출해야 함. (python interator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.327669Z",
     "start_time": "2017-10-11T01:45:33.321357Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, '756+55', 4, '=811', '811 ')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_len,e_in,d_len,d_in,d_out = train_data.next_batch().next()\n",
    "decode_data(e_len[0],e_in[0],d_len[0],d_in[0],d_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.340346Z",
     "start_time": "2017-10-11T01:45:33.331423Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, '269+927', 5, '=1196', '1196 ')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_data(e_len[1],e_in[1],d_len[1],d_in[1],d_out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow - build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.345533Z",
     "start_time": "2017-10-11T01:45:33.343653Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow - placeholders & dynamic batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.385906Z",
     "start_time": "2017-10-11T01:45:33.346950Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs   = tf.placeholder(\n",
    "    dtype=tf.float32,\n",
    "    shape=[None, encoder_max_seq_len, input_units],\n",
    "    name='encoder_inputs')\n",
    "encoder_seqlen   = tf.placeholder(\n",
    "    dtype=tf.int32,\n",
    "    shape=[None],\n",
    "    name='encoder_seqlen')\n",
    "decoder_inputs   = tf.placeholder(\n",
    "    dtype=tf.float32,\n",
    "    shape=[None, decoder_max_seq_len, input_units],\n",
    "    name='decoder_inputs')\n",
    "decoder_targets  = tf.placeholder(\n",
    "    dtype=tf.float32,\n",
    "    shape=[None, decoder_max_seq_len, output_units],\n",
    "    name='decoder_targets')\n",
    "decoder_seqlen   = tf.placeholder(\n",
    "    dtype=tf.int32,\n",
    "    shape=[None],\n",
    "    name='decoder_seqlen')\n",
    "encoder_training = tf.placeholder(\n",
    "    dtype=tf.bool,\n",
    "    shape=None,\n",
    "    name='encoder_training')\n",
    "tf_batch_size = tf.shape(encoder_inputs)[0] # <<== !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.401416Z",
     "start_time": "2017-10-11T01:45:33.387467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN, Encoder/Decoder - 2-Layers, with Dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`tf.cond()`](http://devdocs.io/tensorflow~python/tf/cond)\n",
    "> Return true_fn() if the predicate pred is true else false_fn()\n",
    "\n",
    "```\n",
    "cond(\n",
    "    pred,\n",
    "    true_fn=None,\n",
    "    false_fn=None,\n",
    "    strict=False,\n",
    "    name=None,\n",
    "    fn1=None,\n",
    "    fn2=None\n",
    ")\n",
    "```\n",
    "\n",
    "- `true_fn`, `false_fn` 에는 [lambda expression](https://docs.python.org/2.7/reference/expressions.html#lambda) 이 종종 사용됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:33.409312Z",
     "start_time": "2017-10-11T01:45:33.403593Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "keep_prob = tf.cond(encoder_training,\n",
    "                    lambda: tf.constant(1.0-dropout_rate),\n",
    "                    lambda: tf.constant(1.0),\n",
    "                    name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`tf.contrib.rnn.DropoutWrapper`](http://devdocs.io/tensorflow~python/tf/contrib/rnn/dropoutwrapper)\n",
    "\n",
    "<code>\n",
    "    `__init__`(\n",
    "        <span style=\"color:red\">cell,</span>\n",
    "        input_keep_prob=1.0,\n",
    "        output_keep_prob=1.0,\n",
    "        state_keep_prob=1.0,\n",
    "        <span style=\"color:red\">variational_recurrent=False,</span>\n",
    "        input_size=None,\n",
    "        dtype=None,\n",
    "        seed=None\n",
    "    )\n",
    "</code>\n",
    "\n",
    "\n",
    "- `dropout_prob` 이 아니라 `keep_prob` 를 적어주는 점에 유의\n",
    "\n",
    "- `input_keep_prob`, `output_keep_prob`, `state_keep_prob` 로 구분하여 적용\n",
    "\n",
    "- `variational_recurrent` 플래그 (**_tensorflow_** 1.1 부터 지원)\n",
    "\n",
    "  - [A Theorerically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
    "\n",
    "  - RNN의 경우, 훈련데이터에 overfitting 하는 경향이 심한데, 일반적인 dropout 방식을 사용해도 효과가 없더라\n",
    "  \n",
    "  - Bayesian interpretation 으로 dropout 기법에 대해서 분석해 본 결과 RNN 에 적용 가능한 새로운 dropout 기법을 개발\n",
    "\n",
    "\n",
    "- [`tf.contrib.rnn.LayerNormBasicLSTMCell`](http://devdocs.io/tensorflow~python/tf/contrib/rnn/layernormbasiclstmcell) 에는 자체 [_recurrent dropout_](https://arxiv.org/abs/1603.05118) 지원 기능이 있음\n",
    "\n",
    "```\n",
    "    __init__(\n",
    "        num_units,\n",
    "        forget_bias=1.0,\n",
    "        input_size=None,\n",
    "        activation=tf.tanh,\n",
    "        layer_norm=True,\n",
    "        norm_gain=1.0,\n",
    "        norm_shift=0.0,\n",
    "        dropout_keep_prob=1.0,\n",
    "        dropout_prob_seed=None,\n",
    "        reuse=None\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> BasicRNNCell, BasicLSTMCell, GRUCell 모두 state 의 형태가 다르다 </span>\n",
    "\n",
    "#### cell initial state 를 생성하기 위해서는 [`cell.zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell#zero_state) 메소드 사용\n",
    "\n",
    "<code>\n",
    "    zero_state(\n",
    "        batch_size,\n",
    "        dtype\n",
    "    )\n",
    "</code>\n",
    "\n",
    "- batch_size: int, float, or <span style=\"color:red\">unit Tensor</span> representing the batch size.\n",
    "\n",
    "- dtype: the data type to use for the state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-RNN 과 Decoder-RNN 은 `initial_state` 를 통해서 연결된다\n",
    "\n",
    "- Encoder-RNN\n",
    "<code>\n",
    "    encoder_out, <span style=\"color:red\">encoder_state</span> = tf.nn.dynamic_rnn(...)\n",
    "</code>\n",
    "\n",
    "\n",
    "- DecoderRNN\n",
    "<code>\n",
    "    decoder_out, decoder_state = tf.nn.dynamic_rnn(... <span style=\"color:red\">initial_state=encoder_state</span>)\n",
    "</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.279454Z",
     "start_time": "2017-10-11T01:45:33.411152Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder RNN\n",
    "\n",
    "def e_cell(input_size):\n",
    "#     cell = tf.contrib.rnn.BasicRNNCell(hidden_units)\n",
    "#     cell = tf.contrib.rnn.LSTMCell(hidden_units)\n",
    "#     cell = tf.contrib.rnn.DropoutWrapper(\n",
    "#         cell,\n",
    "#         state_keep_prob = keep_prob,\n",
    "#         variational_recurrent = True,\n",
    "#         input_size = input_size,\n",
    "#         dtype = tf.float32)\n",
    "    cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "             hidden_units,\n",
    "             dropout_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "with tf.variable_scope('encoder'):\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [\n",
    "            e_cell(input_units),\n",
    "            e_cell(hidden_units),\n",
    "            e_cell(hidden_units)\n",
    "        ])\n",
    "    initial_state = cell.zero_state(\n",
    "        batch_size=tf_batch_size,\n",
    "        dtype=tf.float32)\n",
    "    encoder_out, encoder_state = tf.nn.dynamic_rnn(\n",
    "        cell,\n",
    "        encoder_inputs,\n",
    "        sequence_length=encoder_seqlen,\n",
    "        initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.359677Z",
     "start_time": "2017-10-11T01:45:34.280682Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder RNN\n",
    "def d_cell():\n",
    "#     cell = tf.contrib.rnn.BasicRNNCell(hidden_units)\n",
    "#     cell = tf.contrib.rnn.LSTMCell(hidden_units)\n",
    "    cell = tf.contrib.rnn.LayerNormBasicLSTMCell(\n",
    "             hidden_units)\n",
    "    return cell\n",
    "\n",
    "with tf.variable_scope('decoder'):\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [d_cell() for _ in range(3)])\n",
    "    initial_state = encoder_state # <<<==== \n",
    "    decoder_out, decoder_state = tf.nn.dynamic_rnn(\n",
    "        cell,\n",
    "        decoder_inputs,\n",
    "        sequence_length=decoder_seqlen,\n",
    "        initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.365887Z",
     "start_time": "2017-10-11T01:45:34.360977Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 7, 100]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out.shape.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.374602Z",
     "start_time": "2017-10-11T01:45:34.367079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 5, 100]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_out.shape.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Network after RNN\n",
    "\n",
    "- 5주 1일차에 사용한 코드는 이렇지만,\n",
    "\n",
    "\n",
    "<div  style=\"width:45.0rem;margin:auto;border:1px solid black;border-radius:3px\">\n",
    "<code>\n",
    "        # 10 개의 output units 로 만들 \n",
    "        #  FCN (fully-connected-network) 구성\n",
    "        # outputs shape will become: [batch_size, 10]\n",
    "        <span style=\"color:red\">outputs    = tf.layers.dense(rnn_output, 10)</span>\n",
    "\n",
    "</code>\n",
    "</div>\n",
    "\n",
    "- 이번에는 `tf.nn.xw_plus_b()` 를 이용해서 만들어 봅니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.395408Z",
     "start_time": "2017-10-11T01:45:34.376754Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_w    = tf.get_variable(\n",
    "                \"output_w\",\n",
    "                [hidden_units, output_units])\n",
    "output_b    = tf.get_variable(\n",
    "                \"output_b\",\n",
    "                [output_units])\n",
    "\n",
    "# xw_plus_b() 는 2D 텐서만 처리할 수 있음\n",
    "decoder_o_  = tf.reshape(decoder_out,\n",
    "                         [-1, hidden_units])\n",
    "outputs_    = tf.nn.xw_plus_b(decoder_o_,\n",
    "                              output_w,\n",
    "                              output_b)\n",
    "# xw_plus_b() 를 위해 변형했던 것 처럼 출력을 다시 원 형태로 원복\n",
    "outputs     = tf.reshape(\n",
    "                outputs_,\n",
    "                [-1, decoder_max_seq_len, output_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.399472Z",
     "start_time": "2017-10-11T01:45:34.396688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 5, 13]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.486625Z",
     "start_time": "2017-10-11T01:45:34.400676Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    gpu_options={'allow_growth': True})\n",
    "sess = tf.InteractiveSession(config=tf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.668287Z",
     "start_time": "2017-10-11T01:45:34.487881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.856848Z",
     "start_time": "2017-10-11T01:45:34.669578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_len, e_in, d_len, d_in, d_out = \\\n",
    "    train_data.next_batch().next()\n",
    "feed = {\n",
    "    encoder_training: False,\n",
    "    encoder_seqlen: e_len,\n",
    "    encoder_inputs: e_in,\n",
    "    decoder_seqlen: d_len,\n",
    "    decoder_inputs: d_in,\n",
    "    decoder_targets: d_out,\n",
    "}\n",
    "out = sess.run(outputs, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.863270Z",
     "start_time": "2017-10-11T01:45:34.858235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  9, 10, 11,  5,  5,  3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_max(e_in[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.878447Z",
     "start_time": "2017-10-11T01:45:34.864865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, '789+442', 5, '=1231', '1231 ')\n",
      "(7, '253+889', 5, '=1142', '1142 ')\n",
      "(7, '531+142', 4, '=673', '673 ')\n",
      "(6, '745+69', 4, '=814', '814 ')\n",
      "(7, '576+728', 5, '=1304', '1304 ')\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(decode_data(e_len[i],e_in[i],d_len[i],d_in[i],d_out[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare training target vs output\n",
    "\n",
    "- handling sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.895467Z",
     "start_time": "2017-10-11T01:45:34.879844Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_mask_ = tf.sequence_mask(\n",
    "                decoder_seqlen,\n",
    "                maxlen=decoder_max_seq_len,\n",
    "                dtype=tf.float32)\n",
    "\n",
    "seq_mask     = \\\n",
    "    tf.tile(\n",
    "        tf.reshape(\n",
    "            seq_mask_,\n",
    "            [-1,decoder_max_seq_len,1]),\n",
    "        [1,1,output_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.918421Z",
     "start_time": "2017-10-11T01:45:34.896864Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_seq_mask_,a_seq_mask = sess.run(\n",
    "    [\n",
    "        seq_mask_,\n",
    "        seq_mask,\n",
    "    ],\n",
    "    {decoder_seqlen: [1,2,3,4,5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.928262Z",
     "start_time": "2017-10-11T01:45:34.919877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_seq_mask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.938073Z",
     "start_time": "2017-10-11T01:45:34.929613Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_seq_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:34.976555Z",
     "start_time": "2017-10-11T01:45:34.939458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss         = tf.losses.softmax_cross_entropy(\n",
    "                    decoder_targets * seq_mask,\n",
    "                    outputs * seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.446179Z",
     "start_time": "2017-10-11T01:45:34.978409Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer    = tf.train.AdamOptimizer( \\\n",
    "                  learning_rate=0.001)\n",
    "optimize     = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.489016Z",
     "start_time": "2017-10-11T01:45:35.447288Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer가 새로 변수를 만들었을 것이므로\n",
    "# 변수 초기화를 다시 해야 함\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.562258Z",
     "start_time": "2017-10-11T01:45:35.490377Z"
    },
    "cell_style": "center",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e_len, e_in, d_len, d_in, d_out = \\\n",
    "    train_data.next_batch().next()\n",
    "feed = {\n",
    "    encoder_training: True,\n",
    "    encoder_seqlen:   e_len,\n",
    "    encoder_inputs:   e_in,\n",
    "    decoder_seqlen:   d_len,\n",
    "    decoder_inputs:   d_in,\n",
    "    decoder_targets:  d_out,\n",
    "}\n",
    "_, out, loss_value = \\\n",
    "    sess.run([optimize, outputs, loss], feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.570554Z",
     "start_time": "2017-10-11T01:45:35.563427Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 5, 13), (200, 5, 13), 2.3389759)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(d_out).shape, out.shape, loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.581928Z",
     "start_time": "2017-10-11T01:45:35.571768Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_equals(a,b,a_len=None,b_len=None):\n",
    "    if a_len is None: a_len = len(a)\n",
    "    if b_len is None: b_len = len(b)\n",
    "    a_nums = np.argmax(a[:a_len],-1)\n",
    "    b_nums = np.argmax(b[:b_len],-1)\n",
    "    return 1.0 * np.all(np.equal(a_nums, b_nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.592312Z",
     "start_time": "2017-10-11T01:45:35.583111Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-11T01:45:35.619717Z",
     "start_time": "2017-10-11T01:45:35.593609Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, writer):\n",
    "    t_start = time.time()\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        losses  = []\n",
    "        errs    = []\n",
    "        for e_len, e_in, d_len, d_in, d_out \\\n",
    "                in train_data.next_batch():\n",
    "            feed = {\n",
    "                encoder_training: True,\n",
    "                encoder_seqlen:   e_len,\n",
    "                encoder_inputs:   e_in,\n",
    "                decoder_seqlen:   d_len,\n",
    "                decoder_inputs:   d_in,\n",
    "                decoder_targets:  d_out,\n",
    "            }\n",
    "            _, out, training_loss = \\\n",
    "                sess.run([optimize, outputs, loss], feed)\n",
    "            training_err = 1.0 - \\\n",
    "                np.mean([\n",
    "                    seq_equals(a,b,a_len,b_len)\n",
    "                    for a,b,a_len,b_len in\n",
    "                    zip(d_out,out,d_len,d_len)\n",
    "                ])\n",
    "            losses.append(training_loss)\n",
    "            errs.append(training_err)\n",
    "        test_errs   = []\n",
    "        for e_len, e_in, d_len, d_in, d_out \\\n",
    "                in test_data.next_batch():\n",
    "            feed = {\n",
    "                encoder_training: False,\n",
    "                encoder_seqlen:   e_len,\n",
    "                encoder_inputs:   e_in,\n",
    "                decoder_seqlen:   d_len,\n",
    "                decoder_inputs:   d_in,\n",
    "                decoder_targets:  d_out,\n",
    "            }\n",
    "            out, = sess.run([outputs], feed)\n",
    "            test_err = 1.0 - \\\n",
    "                np.mean([\n",
    "                    seq_equals(a,b,a_len,b_len)\n",
    "                    for a,b,a_len,b_len in\n",
    "                    zip(d_out,out,d_len,d_len)\n",
    "                ])\n",
    "            test_errs.append(test_err)\n",
    "        mean_loss       = np.mean(losses)\n",
    "        mean_err        = np.mean(errs)\n",
    "        mean_test_err   = np.mean(test_errs)\n",
    "        summary = tf.Summary(\n",
    "            value=[\n",
    "                tf.Summary.Value(\n",
    "                    tag='loss',\n",
    "                    simple_value=mean_loss),\n",
    "                tf.Summary.Value(\n",
    "                    tag='train_err',\n",
    "                    simple_value=mean_err),\n",
    "                tf.Summary.Value(\n",
    "                    tag='test_err',\n",
    "                    simple_value=mean_test_err),\n",
    "            ])\n",
    "        writer.add_summary(summary,epoch+1)\n",
    "        if 0 == (epoch+1) % 10:\n",
    "            t_elapsed = time.time() - t_start\n",
    "            print(('epoch: {:d}, loss: {:.5f}, ' +\n",
    "                   'err: {:.5f}, test_err: {:.5f}, ' +\n",
    "                   'elapsed: {:.2f}').format(\n",
    "                epoch+1,\n",
    "                mean_loss,\n",
    "                mean_err,\n",
    "                mean_test_err,\n",
    "                t_elapsed))\n",
    "            t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.537Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\n",
    "            'logdir4/encoder_decoder',\n",
    "            tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, loss: 0.51062, err: 0.85138, test_err: 0.80160, elapsed: 320.23\n",
      "epoch: 20, loss: 0.07630, err: 0.12327, test_err: 0.07380, elapsed: 281.61\n",
      "epoch: 30, loss: 0.03691, err: 0.05430, test_err: 0.01070, elapsed: 268.98\n"
     ]
    }
   ],
   "source": [
    "train(100, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training progress was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.541Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !tensorboard --logdir logdir2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained network - prepare test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.543Z"
    }
   },
   "outputs": [],
   "source": [
    "t1, op, t2        = '123', '+', '456'\n",
    "\n",
    "expr              = t1 + op + t2\n",
    "ans               = '='+str(eval(expr))\n",
    "target            = (ans+' ')[1:]\n",
    "\n",
    "print('expr:   ',[c for c in expr])\n",
    "print('ans:    ',[c for c in ans])\n",
    "print('target: ',[c for c in target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained network - prepare test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.545Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_encoder_input = [one_hot(symbol_map[c]) \\\n",
    "                      for c in expr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.546Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(test_encoder_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.548Z"
    }
   },
   "outputs": [],
   "source": [
    "test_encoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test trained network - run encoder\n",
    "\n",
    "- 7 개의 입력 시퀀스를 넣음\n",
    "\n",
    "  - batch size = 1\n",
    "  \n",
    "  - `encoder_seqlen` : [7]\n",
    "\n",
    "  - `encoder_inputs` : `test_encoder_input`\n",
    "    - `'1', '2', '3', '+', '4', '5', '6'` 에 대한 인덱스 값들을 one-hot encoding\n",
    "\n",
    "  - `encoder_training` : False (dropout_rate 를 0.0 으로 설정)\n",
    "  \n",
    "- encoder 의 출력:\n",
    "\n",
    "  - `encoder_out` : encoder RNN 의 매 batch, 매 sequence 마다의 출력. **사용하지않음**\n",
    "  \n",
    "  - `encoder_state` : 입력된 연산식의 계산 결과값에 대한 _representation_ 이 들어 있다고 여겨지는 값\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed = {\n",
    "    encoder_seqlen: [encoder_max_seq_len],\n",
    "    encoder_inputs: [test_encoder_input],\n",
    "    encoder_training: False\n",
    "}\n",
    "e_out, e_state = sess.run(\n",
    "                    [encoder_out, encoder_state],\n",
    "                    feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.551Z"
    }
   },
   "outputs": [],
   "source": [
    "e_out.shape, type(e_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained network - First Decoder Run\n",
    "\n",
    "- `answer` : 결과 값을 문자로 받기 위한 7자 버퍼\n",
    "\n",
    "- `answer` 값이 decoder의 초기 입력으로 제공됨\n",
    "\n",
    "- `answer` 값이 decoder 초기 입력이 될 때, 첫 심볼이 `'='` 이어야 함. 나머지는 don't care\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.553Z"
    }
   },
   "outputs": [],
   "source": [
    "answer = ['='] + [c for c in '    ']\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Inputs/Outputs\n",
    "\n",
    "- `encoder_state` : 계산결과의 _representation_ 이 들어있다고 생각되는, encoder network 의 최종 상태\n",
    "\n",
    "- `decoder_seqlen` : 루프 한 스텝에서 입력할 디코더 시퀀스 길이\n",
    "    \n",
    "- `decoder_inputs` : 루프 한 스텝 분량의 디코더 입력값\n",
    "\n",
    "- `decoder_state` : decoder RNN 의 한 스텝 진행 후의 상태. 다음 스텝을 진행할 때 decoder initial state 로 제시 해야 하는 값\n",
    "\n",
    "- `decoder_out` : decoder RNN 의 직접 출력. **사용하지않음**. 이 값을 FCN 으로 보내 나오는 `outputs` 가 실제 출력\n",
    "\n",
    "- `outputs` : 최종 one-hot output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.554Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed = {\n",
    "    encoder_state: e_state,\n",
    "    decoder_seqlen: [1],\n",
    "    decoder_inputs: [[one_hot(symbol_map[c]) \\\n",
    "                      for c in answer]]\n",
    "}\n",
    "out, d_out, d_state = sess.run(\n",
    "                        [outputs,\n",
    "                         decoder_out,\n",
    "                         decoder_state],\n",
    "                        feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.555Z"
    }
   },
   "outputs": [],
   "source": [
    "out.shape, d_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.556Z"
    }
   },
   "outputs": [],
   "source": [
    "arg_max(out[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.558Z"
    }
   },
   "outputs": [],
   "source": [
    "symbols[arg_max(out[0,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained network - Repeated Decoder Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digit = symbols[arg_max(out[0,0])]\n",
    "collect_answer = [digit]\n",
    "for _ in range(1,decoder_max_seq_len):\n",
    "    feed = {\n",
    "        encoder_state: d_state,\n",
    "        decoder_seqlen: [1],\n",
    "        decoder_inputs: [[one_hot(symbol_map[c]) \\\n",
    "                          for c in answer]]\n",
    "    }\n",
    "    out, d_out, d_state = \\\n",
    "        sess.run([outputs, decoder_out, decoder_state],\n",
    "                 feed)\n",
    "    digit = symbols[arg_max(out[0,0])]\n",
    "    collect_answer.append(digit)\n",
    "    answer[0] = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.561Z"
    }
   },
   "outputs": [],
   "source": [
    "collect_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Wrap-up: infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer(expr):\n",
    "    # encoder\n",
    "    feed = {\n",
    "        encoder_seqlen: [len(expr)],\n",
    "        encoder_inputs: \\\n",
    "            [str_to_onehot(expr,encoder_max_seq_len)],\n",
    "        encoder_training: False\n",
    "    }\n",
    "    e_out, e_state = \\\n",
    "        sess.run([encoder_out, encoder_state], feed)\n",
    "    \n",
    "    # decoder: step 0\n",
    "    out_buf = []\n",
    "    feed = {\n",
    "        encoder_state: e_state,\n",
    "        decoder_seqlen: [1],\n",
    "        decoder_inputs: \\\n",
    "            [str_to_onehot('=',decoder_max_seq_len)]\n",
    "    }\n",
    "    out, d_state = sess.run([outputs, decoder_state], feed)\n",
    "    out_decoded = onehot_to_str(out[0],1)\n",
    "    out_buf.append(out_decoded)\n",
    "    \n",
    "    # decoder: step 1..n-1\n",
    "    for _ in range(1,decoder_max_seq_len):\n",
    "        feed = {\n",
    "            encoder_state: d_state,\n",
    "            decoder_seqlen: [1],\n",
    "            decoder_inputs: \\\n",
    "                [str_to_onehot(out_decoded,decoder_max_seq_len)]\n",
    "        }\n",
    "        out, d_state = sess.run([outputs, decoder_state], feed)\n",
    "        out_decoded = onehot_to_str(out[0],1)\n",
    "        out_buf.append(out_decoded)\n",
    "\n",
    "    return ''.join(out_buf), e_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.563Z"
    }
   },
   "outputs": [],
   "source": [
    "ans, e_out = infer('345+111')\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.565Z"
    }
   },
   "outputs": [],
   "source": [
    "ans, e_out = infer('345+222')\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.566Z"
    }
   },
   "outputs": [],
   "source": [
    "ans, e_out = infer('111+222')\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.567Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errs = []\n",
    "for _ in range(20):\n",
    "    expr, ans_ = make_random_data()\n",
    "    truth = (ans_+' ')[1:6]\n",
    "    ans, e_out  = infer(expr)\n",
    "    print('['+truth+']', '['+ans+']', expr)\n",
    "    errs.append(0 if truth == ans else 1)\n",
    "print('errs: {:.5f}'.format(np.mean(errs,dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN activation visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.569Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.array(e_out.transpose() * 127 + 128,dtype=np.int32),\n",
    "           cmap='gray',\n",
    "           aspect=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.570Z"
    }
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(tf.get_default_session(), 'logdir4/encoder_decoder/save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.571Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l logdir2/encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-10-11T01:45:28.572Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess,tf.train.latest_checkpoint('logdir/rnn1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
